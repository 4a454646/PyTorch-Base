{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/jeff/opencv-course/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "from operator import itemgetter\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "from torchvision import datasets, transforms\n",
                "from torch.optim.lr_scheduler import MultiStepLR\n",
                "\n",
                "from trainer import Trainer, hooks, configuration\n",
                "from trainer.utils import setup_system, patch_configs\n",
                "from trainer.metrics import AccuracyEstimator\n",
                "from trainer.tensorboard_visualizer import TensorBoardVisualizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": true,
                "lines_to_next_cell": 2
            },
            "outputs": [],
            "source": [
                "def get_data(batch_size, data_root='data', num_workers=1):\n",
                "\n",
                "    train_test_transforms = transforms.Compose([\n",
                "        # Resize to 32X32\n",
                "        transforms.Resize((32, 32)),\n",
                "        # this re-scale image tensor values between 0-1. image_tensor /= 255\n",
                "        transforms.ToTensor(),\n",
                "        # subtract mean (0.1307) and divide by variance (0.3081).\n",
                "        # This mean and variance is calculated on training data (verify yourself)\n",
                "        transforms.Normalize((0.1307, ), (0.3081, ))\n",
                "    ])\n",
                "\n",
                "    # train dataloader\n",
                "    train_loader = torch.utils.data.DataLoader(\n",
                "        datasets.MNIST(root=data_root, train=True, download=True, transform=train_test_transforms),\n",
                "        batch_size=batch_size,\n",
                "        shuffle=True,\n",
                "        num_workers=num_workers\n",
                "    )\n",
                "\n",
                "    # test dataloader\n",
                "    test_loader = torch.utils.data.DataLoader(\n",
                "        datasets.MNIST(root=data_root, train=False, download=True, transform=train_test_transforms),\n",
                "        batch_size=batch_size,\n",
                "        shuffle=False,\n",
                "        num_workers=num_workers\n",
                "    )\n",
                "    return train_loader, test_loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "collapsed": true,
                "lines_to_next_cell": 2
            },
            "outputs": [],
            "source": [
                "class LeNet5(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "\n",
                "        self._body = nn.Sequential(\n",
                "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2),\n",
                "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2),\n",
                "        )\n",
                "        self._head = nn.Sequential(\n",
                "            nn.Linear(in_features=16 * 5 * 5, out_features=120), nn.ReLU(inplace=True),\n",
                "            nn.Linear(in_features=120, out_features=84), nn.ReLU(inplace=True),\n",
                "            nn.Linear(in_features=84, out_features=10)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self._body(x)\n",
                "        x = x.view(x.size()[0], -1)\n",
                "        x = self._head(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "collapsed": true,
                "lines_to_next_cell": 2
            },
            "outputs": [],
            "source": [
                "class Experiment:\n",
                "    def __init__(\n",
                "        self,\n",
                "        system_config: configuration.SystemConfig = configuration.SystemConfig(),\n",
                "        dataset_config: configuration.DatasetConfig = configuration.DatasetConfig(),\n",
                "        dataloader_config: configuration.DataloaderConfig = configuration.DataloaderConfig(),\n",
                "        optimizer_config: configuration.OptimizerConfig = configuration.OptimizerConfig()\n",
                "    ):\n",
                "        self.loader_train, self.loader_test = get_data(\n",
                "            batch_size=dataloader_config.batch_size,\n",
                "            num_workers=dataloader_config.num_workers,\n",
                "            data_root=dataset_config.root_dir\n",
                "        )\n",
                "        \n",
                "        setup_system(system_config)\n",
                "\n",
                "        self.model = LeNet5()\n",
                "        self.loss_fn = nn.CrossEntropyLoss()\n",
                "        self.metric_fn = AccuracyEstimator(topk=(1, ))\n",
                "        self.optimizer = optim.SGD(\n",
                "            self.model.parameters(),\n",
                "            lr=optimizer_config.learning_rate,\n",
                "            weight_decay=optimizer_config.weight_decay,\n",
                "            momentum=optimizer_config.momentum\n",
                "        )\n",
                "        self.lr_scheduler = MultiStepLR(\n",
                "            self.optimizer, milestones=optimizer_config.lr_step_milestones, gamma=optimizer_config.lr_gamma\n",
                "        )\n",
                "        self.visualizer = TensorBoardVisualizer()\n",
                "\n",
                "    def run(self, trainer_config: configuration.TrainerConfig) -> dict:\n",
                "\n",
                "        device = torch.device(trainer_config.device)\n",
                "        self.model = self.model.to(device)\n",
                "        self.loss_fn = self.loss_fn.to(device)\n",
                "\n",
                "        model_trainer = Trainer(\n",
                "            model=self.model,\n",
                "            loader_train=self.loader_train,\n",
                "            loader_test=self.loader_test,\n",
                "            loss_fn=self.loss_fn,\n",
                "            metric_fn=self.metric_fn,\n",
                "            optimizer=self.optimizer,\n",
                "            lr_scheduler=self.lr_scheduler,\n",
                "            device=device,\n",
                "            data_getter=itemgetter(0),\n",
                "            target_getter=itemgetter(1),\n",
                "            stage_progress=trainer_config.progress_bar,\n",
                "            get_key_metric=itemgetter(\"top1\"),\n",
                "            visualizer=self.visualizer,\n",
                "            model_saving_frequency=trainer_config.model_saving_frequency,\n",
                "            save_dir=trainer_config.model_dir\n",
                "        )\n",
                "\n",
                "        model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_classification)\n",
                "        self.metrics = model_trainer.fit(trainer_config.epoch_num)\n",
                "        return self.metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def main():\n",
                "    '''Run the experiment\n",
                "    '''\n",
                "    # patch configs depending on cuda availability\n",
                "    dataloader_config, trainer_config = patch_configs(epoch_num_to_set=15)\n",
                "    dataset_config = configuration.DatasetConfig(root_dir=\"data\")\n",
                "    experiment = Experiment(dataset_config=dataset_config, dataloader_config=dataloader_config)\n",
                "    results = experiment.run(trainer_config)\n",
                "\n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[0/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.822 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:06<00:00, 35.10it/s]\n",
                        "[0/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.147 \u001b[6;30;44mAcc:\u001b[0m 95.420: 100%|██████████| 40/40 [00:01<00:00, 32.72it/s]\n",
                        "[1/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.116 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 54.76it/s]15 [00:08<01:54,  8.18s/it]\n",
                        "[1/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.072 \u001b[6;30;44mAcc:\u001b[0m 97.650: 100%|██████████| 40/40 [00:01<00:00, 35.03it/s]\n",
                        "[2/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.078 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 55.68it/s]15 [00:13<01:27,  6.71s/it]\n",
                        "[2/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.062 \u001b[6;30;44mAcc:\u001b[0m 98.090: 100%|██████████| 40/40 [00:01<00:00, 36.86it/s]\n",
                        "[3/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.061 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 53.65it/s]15 [00:19<01:14,  6.18s/it]\n",
                        "[3/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.054 \u001b[6;30;44mAcc:\u001b[0m 98.230: 100%|██████████| 40/40 [00:01<00:00, 33.48it/s]\n",
                        "[4/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.051 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:05<00:00, 47.50it/s]15 [00:25<01:06,  6.05s/it]\n",
                        "[4/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.042 \u001b[6;30;44mAcc:\u001b[0m 98.450: 100%|██████████| 40/40 [00:01<00:00, 24.35it/s]\n",
                        "[5/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.043 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:05<00:00, 47.54it/s]15 [00:32<01:03,  6.39s/it]\n",
                        "[5/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.047 \u001b[6;30;44mAcc:\u001b[0m 98.310: 100%|██████████| 40/40 [00:01<00:00, 36.29it/s]\n",
                        "[6/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.040 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 54.68it/s]15 [00:38<00:57,  6.36s/it]\n",
                        "[6/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.036 \u001b[6;30;44mAcc:\u001b[0m 98.710: 100%|██████████| 40/40 [00:01<00:00, 36.26it/s]\n",
                        "[7/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.032 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 55.12it/s]15 [00:44<00:49,  6.13s/it]\n",
                        "[7/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.035 \u001b[6;30;44mAcc:\u001b[0m 98.840: 100%|██████████| 40/40 [00:01<00:00, 36.60it/s]\n",
                        "[8/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.030 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 55.40it/s]15 [00:49<00:41,  5.96s/it]\n",
                        "[8/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.041 \u001b[6;30;44mAcc:\u001b[0m 98.580: 100%|██████████| 40/40 [00:01<00:00, 36.32it/s]\n",
                        "[9/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.026 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 55.67it/s]15 [00:55<00:35,  5.85s/it]\n",
                        "[9/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.032 \u001b[6;30;44mAcc:\u001b[0m 98.790: 100%|██████████| 40/40 [00:01<00:00, 35.68it/s]\n",
                        "[10/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.024 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 56.38it/s]15 [01:01<00:28,  5.77s/it]\n",
                        "[10/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.040 \u001b[6;30;44mAcc:\u001b[0m 98.680: 100%|██████████| 40/40 [00:01<00:00, 35.41it/s]\n",
                        "[11/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.022 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 55.93it/s]15 [01:06<00:22,  5.70s/it]\n",
                        "[11/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.031 \u001b[6;30;44mAcc:\u001b[0m 99.010: 100%|██████████| 40/40 [00:01<00:00, 36.16it/s]\n",
                        "[12/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.018 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 55.57it/s]15 [01:12<00:16,  5.66s/it]\n",
                        "[12/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.032 \u001b[6;30;44mAcc:\u001b[0m 99.030: 100%|██████████| 40/40 [00:01<00:00, 33.96it/s]\n",
                        "[13/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.016 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 56.17it/s]15 [01:17<00:11,  5.66s/it]\n",
                        "[13/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.030 \u001b[6;30;44mAcc:\u001b[0m 99.050: 100%|██████████| 40/40 [00:01<00:00, 34.13it/s]\n",
                        "[14/15][Train] \u001b[6;30;42mloss:\u001b[0m 0.016 \u001b[6;30;42mLR:\u001b[0m   0.005: 100%|██████████| 240/240 [00:04<00:00, 55.87it/s]15 [01:23<00:05,  5.66s/it]\n",
                        "[14/15][Test ] \u001b[6;30;44mloss:\u001b[0m 0.031 \u001b[6;30;44mAcc:\u001b[0m 99.010: 100%|██████████| 40/40 [00:01<00:00, 34.53it/s]\n",
                        "\u001b[6;30;45mtest_top1:\u001b[0m 99.010 \u001b[6;30;45mtrain_loss:\u001b[0m 0.016 \u001b[6;30;45mtest_loss:\u001b[0m 0.031 : 100%|██████████| 15/15 [01:29<00:00,  5.94s/it]\n"
                    ]
                }
            ],
            "source": [
                "if __name__ == '__main__':\n",
                "    main()\n",
                "    "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.4 ('opencv-course')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "vscode": {
            "interpreter": {
                "hash": "cc378a7666fd3eee2c39db92d67f0f5595e9050382902d6af70c22ea2e477f8d"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
